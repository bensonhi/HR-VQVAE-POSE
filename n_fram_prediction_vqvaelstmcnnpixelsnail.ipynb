{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from video.dataset import MnistVideoCodeLMDBDataset\n",
    "from video.dataloader import video_mnist_dataloader\n",
    "from torchvision import utils\n",
    "import numpy as np\n",
    "from video.LSTM3 import LSTM3\n",
    "from video.LSTM_PixelSnail import LSTM_PixelSnail\n",
    "from image.modified.m_vqvae import VQVAE_1\n",
    "from torch import nn\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from image.modified.m_pixelsnail import PixelSNAIL\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_name = 'vqvae_videomnist_2_00099'\n",
    "vqvae_ckpt_path = '../video/checkpoints/videomnist/vqvae/1/00099.pt'\n",
    "\n",
    "input_channel = 16\n",
    "hidden_channel = input_channel\n",
    "epoch_num = 100\n",
    "batch_size = 8\n",
    "device = 'cuda'\n",
    "lr = 0.0004\n",
    "run_num = 1\n",
    "image_samples = 1\n",
    "frame_learn = 8\n",
    "frame_pred = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MnistVideoCodeLMDBDataset(lambda_name, 20)\n",
    "loader = video_mnist_dataloader(dataset, batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_model = VQVAE_1(in_channel=1,\n",
    "            channel=32,\n",
    "            n_res_block=4,\n",
    "            n_res_channel=16,\n",
    "            embed_dim=16,\n",
    "            n_embed=input_channel,\n",
    "            decay=0.99, )\n",
    "vqvae_model = nn.DataParallel(vqvae_model)\n",
    "vqvae_model.load_state_dict(torch.load(vqvae_ckpt_path))\n",
    "vqvae_model = vqvae_model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videomnist_path = '../video/datasets/mnist/moving_mnist/mnist_test_seq.npy'\n",
    "orginal_frames = np.load(videomnist_path)\n",
    "orginal_frames = orginal_frames.swapaxes(0, 1).astype(np.float32)\n",
    "orginal_frames[orginal_frames > 0] = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (16,16)\n",
    "input_channel = 16\n",
    "device = 'cuda'\n",
    "\n",
    "hidden_channel = 256\n",
    "cnn_channel = 256\n",
    "channel = 256\n",
    "cnn_kernel_size = 5\n",
    "kernel_size = 5\n",
    "n_block = 3\n",
    "n_res_block = 3\n",
    "n_res_channel = 246\n",
    "dropout = 0.1\n",
    "n_out_res_block = 3\n",
    "n_cond_res_block = 3\n",
    "cond_res_channel = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTM3( input_channel= input_channel,hidden_channel= hidden_channel, device=device)\n",
    "\n",
    "cnn_model = nn.Conv2d(hidden_channel,\n",
    "            cnn_channel,\n",
    "            cnn_kernel_size,\n",
    "            stride=1,\n",
    "            padding=cnn_kernel_size // 2,)\n",
    "            \n",
    "pixel_model = PixelSNAIL(\n",
    "            shape = [input_size[0], input_size[1]],\n",
    "            n_class = input_channel,\n",
    "            cond_channel = cnn_channel,\n",
    "            channel = channel,\n",
    "            kernel_size = kernel_size,\n",
    "            n_block = n_block,\n",
    "            n_res_block = n_res_block,\n",
    "            res_channel = n_res_channel,\n",
    "            dropout=dropout,\n",
    "            n_out_res_block=n_out_res_block,\n",
    "            n_cond_res_block=n_cond_res_block,\n",
    "            cond_res_channel=cond_res_channel,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmpixelsnail_ckpt_path = '../video/checkpoints/videomnist/vqvae-lstm-pixelsnail/1/00013.pt'\n",
    "\n",
    "lstmpixelsnail_model = LSTM_PixelSnail(lstm_model,cnn_model,pixel_model)\n",
    "lstmpixelsnail_model = nn.DataParallel(lstmpixelsnail_model)\n",
    "\n",
    "lstmpixelsnail_model.load_state_dict(torch.load(lstmpixelsnail_ckpt_path))\n",
    "\n",
    "lstmpixelsnail_model = lstmpixelsnail_model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vqvae_decode(sample):\n",
    "    sample = vqvae_model.module.decode_code(sample)\n",
    "    sample = sample.cpu().detach()\n",
    "    sample = sample.squeeze()\n",
    "    sample = (sample > 0.5).float()\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(sample):\n",
    "    sample = get_vqvae_decode(sample)\n",
    "    plt.imshow(sample[0,:,:])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_learn(lstm_model, cnn_model,pixel_model , inputs, cells_state = None):\n",
    "    \n",
    "    outs = []\n",
    "    size = inputs.size()\n",
    "    for i in range(size[1]):\n",
    "        lstm_out, cells_state = lstm_model(inputs[:,i,:,:,:], cells_state) \n",
    "        outs.append(lstm_out)\n",
    "#     state = []\n",
    "#     for state in cells_state:\n",
    "#         states.append([state[0].detach(),state[1].detach()])\n",
    "        \n",
    "    return outs, cells_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_one_hot(y, num_classes):\n",
    "    scatter_dim = len(y.size())\n",
    "    y_tensor = y.view(*y.size(), -1)\n",
    "    zeros = torch.zeros(*y.size(), num_classes, dtype=y.dtype)\n",
    "\n",
    "    return zeros.scatter(scatter_dim, y_tensor, 1).permute(0, 3, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_to_int(y):\n",
    "    print('y {}'.format(y.size()))\n",
    "\n",
    "    y_trans = y.permute(0, 2, 3, 1)\n",
    "    y_trans = y_trans.argmax(dim=-1)\n",
    "    return y_trans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(sample,frame):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        vqvae_model.eval()\n",
    "        sample_decode = get_vqvae_decode(sample)\n",
    "        frame_decode = get_vqvae_decode(frame)\n",
    "#         sample = vqvae_model.module.decode_code(sample)\n",
    "#         sample = sample.cpu().detach()\n",
    "#         sample = sample.squeeze()\n",
    "#         sample = (sample > 0.5).float()\n",
    "        \n",
    "#         frame = vqvae_model.module.decode_code(frame)\n",
    "#         frame = frame.cpu().detach()\n",
    "#         frame = frame.squeeze()\n",
    "#         frame = (frame > 0.5).float()\n",
    "    \n",
    "#         merge = torch.cat([sample,frame], 0)\n",
    "#         utils.save_image(\n",
    "#             merge,\n",
    "#             'nframe_pred_{}.png'.format(*[run_num]),\n",
    "#             nrow=len_pred,\n",
    "# #             normalize=False,\n",
    "# #             range=(-1, 1),\n",
    "#         )\n",
    "#         img = plt.imread('nframe_pred_{}.png'.format(*[run_num]))\n",
    "        plt.imshow(sample_decode[0,:,:])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel_snail_out(cnn_out, size,temperature=1.0):\n",
    "    cache = {}\n",
    "    row = torch.zeros( *size).to('cuda')\n",
    "    for i in tqdm(range(size[2])):\n",
    "        for j in range(size[3]):\n",
    "            out, cache = pixel_model(row[: ,:, : i + 1, :], condition=cnn_out, cache=cache)\n",
    "            prob = torch.softmax(out[:, :, i, j] / temperature, 1)\n",
    "            sample = torch.argmax(prob, 1, keepdim=False)\n",
    "            sample = cuda_to_one_hot(sample, size[1]).float()\n",
    "\n",
    "            row[:,:, i, j] = sample\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cuda_to_one_hot(y, num_classes):\n",
    "    scatter_dim = len(y.size())\n",
    "    y_tensor = y.view(*y.size(), -1)\n",
    "    zeros = torch.zeros(*y.size(), num_classes, dtype=y.dtype).to('cuda')\n",
    "\n",
    "    return zeros.scatter(scatter_dim, y_tensor, 1)\n",
    "\n",
    "def get_sample(lstm_model, cnn_model,pixel_model ,  input_ , cells_state, temperature=1.0):\n",
    "    \n",
    "    size = input_.size()\n",
    "    \n",
    "\n",
    "    lstm_out, cells_state = lstm_model(input_, cells_state)\n",
    "    \n",
    "    cnn_out = cnn_model(lstm_out)\n",
    "    raw = get_pixel_snail_out(cnn_out, size)\n",
    "\n",
    "    return row, cells_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual(outs,ins):\n",
    "    size = ins[:,0,:,:,:].size()\n",
    "    cnn_model.eval()\n",
    "    pixel_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for out in outs:\n",
    "            cnn_out = cnn_model(out)\n",
    "            raw = get_pixel_snail_out(cnn_out, size) \n",
    "            sample_decode = get_vqvae_decode(one_hot_to_int(raw))\n",
    "            plt.imshow(sample_decode[0,:,:])\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmpixelsnail_model.eval()\n",
    "preds =[]\n",
    "torch.backends.cudnn.enabled = False\n",
    "lstm_model.eval()\n",
    "cnn_model.eval()\n",
    "pixel_model.eval() \n",
    "with torch.no_grad():\n",
    "    for iter_, (frames, video_inds, frame_inds) in enumerate(loader):\n",
    "        if iter_ ==10:\n",
    "            inputs_ = []\n",
    "            f0 = torch.zeros(frames.shape[0], 1, input_channel,frames.shape[2], frames.shape[3])\n",
    "            f0 = f0.to(device)\n",
    "            inputs_.append(f0)\n",
    "            \n",
    "            for i in range(frames.shape[1]):\n",
    "\n",
    "                input_ = _to_one_hot(frames[:, i, :, :], input_channel).float()\n",
    "                input_ = input_.to(device)\n",
    "                inputs_.append(input_.unsqueeze(dim=1))\n",
    "\n",
    "            inputs_ = torch.cat(inputs_, dim=1)\n",
    "            \n",
    "            \n",
    "            outs, states = model_learn(lstm_model, cnn_model,pixel_model , inputs_[:,:frame_learn,:,:,:] )\n",
    "            visual(outs,inputs_)\n",
    "#             samples = None\n",
    "#             sample = inputs_[:,frame_learn,:,:,:]\n",
    "#             for i in range(frame_learn,frame_pred+frame_learn):\n",
    "#                 new_sample,states = get_sample(lstm_model, cnn_model,pixel_model ,sample , states )\n",
    "#                 pred = one_hot_to_int(new_sample)\n",
    "#                 print(sample.size())\n",
    "#                 print(new_sample.size())\n",
    "#                 frames = frames.to('cuda')\n",
    "#                 callback(pred[:,:,:],frames[:,i,:,:])\n",
    "                \n",
    "#                 if samples == None:\n",
    "#                     samples = sample.unsqueeze(1)\n",
    "#                 else:\n",
    "#                     samples = torch.cat((samples,sample.unsqueeze(1)),dim=1)\n",
    "                \n",
    "#                 preds.append(sample)\n",
    "                \n",
    "# #                 states = model_learn(lstm_model, cnn_model,pixel_model , new_sample.unsqueeze(dim=0) ,states)\n",
    "                \n",
    "#                 sample = new_sample\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_int = one_hot_to_int(samples[0,:,:,:])\n",
    "torch.backends.cudnn.enabled = False\n",
    "callback(sample_int,frames[0,frame_learn:frame_pred+frame_learn,:,:],frame_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples[0,:,:,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = inputs_[:,1,:,:,:]\n",
    "# _input =_input.to('cuda')\n",
    "# one = _to_one_hot(_input,16)\n",
    "rel = one_hot_to_int(_input)\n",
    "print(rel)\n",
    "print(frames[:, 0, :, :])\n",
    "# print(_input.shape)\n",
    "# for i in range(_input.shape[1]):\n",
    "#     print(_input[:,i,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(4, 4)\n",
    "print(a)\n",
    "torch.argmax(a, dim=0,keepdim=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
